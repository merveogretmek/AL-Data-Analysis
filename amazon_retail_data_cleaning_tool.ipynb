{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f868b5a",
   "metadata": {},
   "source": [
    "# Amazon Data Cleaning Automation Tool\n",
    "\n",
    "## Overview\n",
    "This notebook automates the process of cleaning and transforming data for Amazon product listings. The objective is to ensure the data meets required quality standards before analysis and reporting. \n",
    "\n",
    "Key tasks include:\n",
    "- Removing duplicates\n",
    "- Handling missing values\n",
    "- Normalizing product names\n",
    "- Ensuring consistent date formats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d439e4df",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eee001df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from ast import literal_eval\n",
    "from itertools import repeat\n",
    "from datetime import datetime\n",
    "import os\n",
    "from openpyxl import Workbook\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import PatternFill, Border, Side, Alignment, Protection, Font, Fill\n",
    "from openpyxl.styles import numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2cf5c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloads_folder = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\n",
    "# List of CSV file paths\n",
    "csv_filenames = [\n",
    "    \"../data/raw/www.amazon.com_20250313_153438.csv\",\n",
    "    \"../data/raw/www.amazon.com_20250313_201257.csv\",\n",
    "    # Add all file paths here\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "# Generate full file paths for each CSV\n",
    "csv_file_paths = [os.path.join(filename) for filename in csv_filenames]\n",
    "# Loop through each file in the provided list and read it into a DataFrame\n",
    "for csv_file_path in csv_file_paths:\n",
    "    try:\n",
    "        dfcon = pd.read_csv(csv_file_path)\n",
    "        dataframes.append(dfcon)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {csv_file_path}: {e}\")\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "if dataframes:\n",
    "    df = pd.concat(dataframes, ignore_index=True)\n",
    "else:\n",
    "    df = pd.DataFrame()  # Empty DataFrame if no valid files are found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebcd24aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current date in YYYYMMDD format\n",
    "current_date = datetime.now().strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f56875a",
   "metadata": {},
   "source": [
    "# Section: Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dfa6a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1p/063yl0ms01z8skkqfmk94z9c0000gn/T/ipykernel_77562/685958155.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['ASIN'].fillna('None', inplace=True)\n",
      "/var/folders/1p/063yl0ms01z8skkqfmk94z9c0000gn/T/ipykernel_77562/685958155.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['sellers'].fillna('[]', inplace=True)\n",
      "/var/folders/1p/063yl0ms01z8skkqfmk94z9c0000gn/T/ipykernel_77562/685958155.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['product_information'].fillna('None', inplace=True)\n",
      "/var/folders/1p/063yl0ms01z8skkqfmk94z9c0000gn/T/ipykernel_77562/685958155.py:43: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['bullet_points'].fillna('None', inplace=True)\n",
      "/var/folders/1p/063yl0ms01z8skkqfmk94z9c0000gn/T/ipykernel_77562/685958155.py:44: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['combination'].fillna('None', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Renaming columns\n",
    "df = df.rename(columns={'Listing_price': 'regular_retail_price', 'promo_price': 'discounted_retail_price'})\n",
    "\n",
    "# Clean the dataset to fix the issue\n",
    "def clean_prices(df):\n",
    "    # If `regular_retail_price` is missing, but `discounted_retail_price` exists, move the value\n",
    "    df['regular_retail_price'] = df.apply(\n",
    "        lambda row: row['discounted_retail_price'] if pd.isna(row['regular_retail_price']) and not pd.isna(row['discounted_retail_price']) else row['regular_retail_price'],\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Set `discounted_retail_price` to NaN where there is no discount (regular price == discounted price)\n",
    "    df['discounted_retail_price'] = df.apply(\n",
    "        lambda row: np.nan if row['regular_retail_price'] == row['discounted_retail_price'] else row['discounted_retail_price'],\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the cleaning function to the dataset\n",
    "df = clean_prices(df)\n",
    "\n",
    "# Handling missing values\n",
    "df['ASIN'].fillna('None', inplace=True)\n",
    "df['sellers'].fillna('[]', inplace=True)\n",
    "\n",
    "# Dropping unnecessary columns\n",
    "columns_to_drop = ['images', 'warning', 'error', 'error_code', 'job_id', 'collector_id']\n",
    "df = df.drop(columns=columns_to_drop, axis=1)\n",
    "\n",
    "# Cleaning specific columns\n",
    "for i in range(len(df)):\n",
    "    try:\n",
    "        df.loc[i, 'bullet_points'] = df.loc[i, 'bullet_points'].replace(']', \"\").replace('[', \"\")\n",
    "        df.loc[i, 'product_information'] = df.loc[i, 'product_information'].replace('{', \"\").replace('}', \"\")\n",
    "        df.loc[i, 'combination'] = df.loc[i, 'combination'].replace('{', \"\").replace('}', \"\").replace(']', \"\").replace('[', \"\").replace(\"name\", \"Option\")\n",
    "        df.loc[i, 'sellers'] = df.loc[i, 'sellers'].replace('UnbeatableSale, Inc', \"UnbeatableSale\")\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Filling NaN values in specific columns\n",
    "df['product_information'].fillna('None', inplace=True)\n",
    "df['bullet_points'].fillna('None', inplace=True)\n",
    "df['combination'].fillna('None', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bec88b",
   "metadata": {},
   "source": [
    "# Section: Processing Product Information Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47428cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1p/063yl0ms01z8skkqfmk94z9c0000gn/T/ipykernel_77562/3361797429.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  product_information['product_information'].fillna('{}', inplace=True)\n",
      "/var/folders/1p/063yl0ms01z8skkqfmk94z9c0000gn/T/ipykernel_77562/3361797429.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  product_information['product_information'].fillna('{}', inplace=True)\n",
      "/var/folders/1p/063yl0ms01z8skkqfmk94z9c0000gn/T/ipykernel_77562/3361797429.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  product_information['product_information'] = product_information['product_information'].replace(\"Assembly required\", \"Assembly Required\", regex=True)\n",
      "/var/folders/1p/063yl0ms01z8skkqfmk94z9c0000gn/T/ipykernel_77562/3361797429.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  product_information['product_information'] = product_information['product_information'].replace(\"Number of pieces\", \"Number of Pieces\", regex=True)\n",
      "/var/folders/1p/063yl0ms01z8skkqfmk94z9c0000gn/T/ipykernel_77562/3361797429.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  product_information['product_information'] = product_information['product_information'].replace(\"Seat height\", \"Seat Height\", regex=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-dict data at index 7014\n",
      "Non-dict data at index 7015\n",
      "Non-dict data at index 7016\n",
      "Non-dict data at index 7017\n",
      "Non-dict data at index 7018\n",
      "Non-dict data at index 7019\n",
      "Non-dict data at index 7020\n",
      "Non-dict data at index 7021\n",
      "Non-dict data at index 7022\n",
      "Non-dict data at index 7023\n",
      "Non-dict data at index 7024\n",
      "Non-dict data at index 7025\n",
      "Non-dict data at index 7026\n",
      "Non-dict data at index 7027\n",
      "Non-dict data at index 7028\n",
      "Non-dict data at index 7029\n",
      "Non-dict data at index 7030\n",
      "Non-dict data at index 7031\n",
      "Non-dict data at index 7032\n",
      "Non-dict data at index 7033\n",
      "Non-dict data at index 7034\n",
      "Non-dict data at index 7035\n",
      "Non-dict data at index 7036\n",
      "Non-dict data at index 7037\n",
      "Non-dict data at index 7038\n",
      "Non-dict data at index 7039\n",
      "Non-dict data at index 7040\n",
      "Non-dict data at index 7041\n",
      "Non-dict data at index 7042\n",
      "Non-dict data at index 7043\n",
      "Non-dict data at index 7044\n",
      "Non-dict data at index 7045\n",
      "Non-dict data at index 7046\n",
      "Non-dict data at index 7047\n",
      "Non-dict data at index 7048\n",
      "Non-dict data at index 7049\n",
      "Non-dict data at index 7050\n",
      "Non-dict data at index 7051\n",
      "Non-dict data at index 7052\n",
      "Non-dict data at index 7053\n",
      "Non-dict data at index 7054\n",
      "Non-dict data at index 7055\n",
      "Non-dict data at index 7056\n",
      "Non-dict data at index 7057\n",
      "Non-dict data at index 7058\n",
      "Non-dict data at index 7059\n",
      "Non-dict data at index 7060\n",
      "Non-dict data at index 7061\n",
      "Non-dict data at index 7062\n",
      "Non-dict data at index 7063\n",
      "Non-dict data at index 7064\n",
      "Non-dict data at index 7065\n",
      "Non-dict data at index 7066\n",
      "Non-dict data at index 7067\n",
      "Non-dict data at index 7068\n",
      "Non-dict data at index 7069\n",
      "Non-dict data at index 7070\n",
      "Non-dict data at index 7071\n",
      "Non-dict data at index 7072\n",
      "Non-dict data at index 7073\n",
      "Non-dict data at index 7074\n",
      "Non-dict data at index 7075\n",
      "Non-dict data at index 7076\n",
      "Non-dict data at index 7077\n",
      "Non-dict data at index 7078\n",
      "Non-dict data at index 7079\n",
      "Non-dict data at index 7080\n",
      "Non-dict data at index 7081\n",
      "Non-dict data at index 7082\n",
      "Non-dict data at index 7083\n",
      "Non-dict data at index 7084\n",
      "Non-dict data at index 7085\n",
      "Non-dict data at index 7086\n",
      "Non-dict data at index 7087\n",
      "Non-dict data at index 7088\n",
      "Non-dict data at index 7089\n",
      "Non-dict data at index 7090\n",
      "Non-dict data at index 7091\n",
      "Non-dict data at index 7092\n"
     ]
    }
   ],
   "source": [
    "# Extracting and cleaning 'product_information' column\n",
    "product_information = df[['ASIN', 'product_information']]\n",
    "product_information['product_information'].fillna('{}', inplace=True)\n",
    "\n",
    "# Standardizing certain key terms in the product information\n",
    "product_information['product_information'] = product_information['product_information'].replace(\"Assembly required\", \"Assembly Required\", regex=True)\n",
    "product_information['product_information'] = product_information['product_information'].replace(\"Number of pieces\", \"Number of Pieces\", regex=True)\n",
    "product_information['product_information'] = product_information['product_information'].replace(\"Seat height\", \"Seat Height\", regex=True)\n",
    "\n",
    "# Initialize a list to hold the processed dataframes\n",
    "dataframes = []\n",
    "\n",
    "# Iterate over rows to convert product information into individual dataframes\n",
    "for index, row in product_information.iterrows():\n",
    "    try:\n",
    "        # Clean any non-dictionary-like formats (if necessary)\n",
    "        product_info = row['product_information']\n",
    "        \n",
    "        # Enclose the string in curly braces if they are missing\n",
    "        if not (product_info.startswith('{') and product_info.endswith('}')):\n",
    "            product_info = '{' + product_info.strip() + '}'\n",
    "        \n",
    "        # Safely evaluate the string representation of a dictionary\n",
    "        data_dict = literal_eval(product_info)\n",
    "        \n",
    "        # If data_dict is a dictionary, proceed to create a DataFrame\n",
    "        if isinstance(data_dict, dict):\n",
    "            dF = pd.DataFrame(data_dict, index=[row['ASIN']])\n",
    "            dataframes.append(dF)\n",
    "        else:\n",
    "            print(f\"Non-dict data at index {index}\")\n",
    "    \n",
    "    except (ValueError, SyntaxError) as e:\n",
    "        print(f\"Error converting to dictionary at index {index}: {e}\")\n",
    "        # Handle the error (optional: append NaN or empty DataFrame)\n",
    "\n",
    "# Concatenate all the DataFrames into one combined DataFrame\n",
    "if dataframes:\n",
    "    pi_combined = pd.concat(dataframes)\n",
    "else:\n",
    "    print(\"No valid product information found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c58870",
   "metadata": {},
   "source": [
    "# Section: Processing Sellers and Product Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d350d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1p/063yl0ms01z8skkqfmk94z9c0000gn/T/ipykernel_77562/1340235633.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_new['sellers'] = df_new['sellers'].map(lambda x: converter_with_error_handling(x), na_action='ignore')\n"
     ]
    }
   ],
   "source": [
    "# Extract relevant columns and define the converter function for 'sellers'\n",
    "df_new = df[['sellers', 'ASIN']]\n",
    "\n",
    "# Define the converter function with improved error handling\n",
    "def converter_with_error_handling(record: str):\n",
    "    try:\n",
    "        if isinstance(record, str):\n",
    "            # Correctly handle boolean values and parse the JSON\n",
    "            record = record.replace(\"true\", \"True\").replace(\"false\", \"False\")\n",
    "            parsed_data = ast.literal_eval(record)\n",
    "            if isinstance(parsed_data, list):\n",
    "                return parsed_data\n",
    "            elif isinstance(parsed_data, dict):\n",
    "                return [parsed_data]\n",
    "            else:\n",
    "                return []  # Return empty list for unexpected data structures\n",
    "        else:\n",
    "            return []  # Return empty list for non-string records\n",
    "    except Exception:\n",
    "        return []  # Return empty list for rows with errors\n",
    "\n",
    "\n",
    "# Apply the converter function to clean the 'sellers' column\n",
    "df_new['sellers'] = df_new['sellers'].map(lambda x: converter_with_error_handling(x), na_action='ignore')\n",
    "\n",
    "# Filter rows with valid 'sellers' data\n",
    "valid_rows = df_new[df_new['sellers'].apply(lambda x: isinstance(x, list) and len(x) > 0)]\n",
    "\n",
    "# Process the valid rows to expand the sellers data\n",
    "dataframeS = []\n",
    "for i in valid_rows.index:\n",
    "    try:\n",
    "        # Repeat ASINs for the number of sellers\n",
    "        asin_list = list(repeat(valid_rows.loc[i, 'ASIN'], len(valid_rows.loc[i, 'sellers'])))\n",
    "        # Convert sellers list into a DataFrame\n",
    "        target = pd.DataFrame(valid_rows.loc[i, 'sellers'], index=asin_list).iloc[::-1]  # Reverse order\n",
    "        dataframeS.append(target)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {i}: {e}\")\n",
    "\n",
    "# Concatenate all processed DataFrames\n",
    "if dataframeS:\n",
    "    total = pd.concat(dataframeS)\n",
    "    total.index.name = 'ASIN'\n",
    "    total = total.rename(columns={\"name\": \"seller_name\"})\n",
    "    total = total.reset_index()\n",
    "else:\n",
    "    total = pd.DataFrame()  # Empty DataFrame if no valid rows\n",
    "\n",
    "# Display the final processed DataFrame\n",
    "#print(total)\n",
    "\n",
    "# Merging with main DataFrame and product information\n",
    "df = pd.merge(df, total, on='ASIN', how='left')\n",
    "df = pd.merge(df, pi_combined, on='ASIN', how='left')\n",
    "\n",
    "# Selecting and renaming columns\n",
    "df = df.rename(columns={'price': 'seller_price'})\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Exporting to Excel\n",
    "filename = f\"amazon_retail_data_{current_date}.xlsx\"\n",
    "# Save the DataFrame to Excel with the new filename\n",
    "df.to_excel(f\"../data/processed/{filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c739550",
   "metadata": {},
   "source": [
    "# Excel Formatting and Styling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7315ff37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Styled Excel file saved to: ../data/processed/amazon_retail_data_20250313_styled.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Load the workbook and select the active worksheet\n",
    "wb = load_workbook(filename=f\"../data/processed/{filename}\")\n",
    "ws = wb.active\n",
    "\n",
    "# Apply auto filter to the worksheet\n",
    "ws.auto_filter.ref = ws.dimensions\n",
    "\n",
    "# Define common styles\n",
    "font = Font(size=15, bold=True)\n",
    "wrap_alignment = Alignment(wrapText=True)\n",
    "left_alignment = Alignment(horizontal='left')\n",
    "fill = PatternFill(\"solid\", fgColor=\"00CCFFCC\")\n",
    "thin_border = Border(\n",
    "    top=Side(border_style='thin', color=\"FF000000\"),\n",
    "    bottom=Side(border_style='thin', color=\"FF000000\"),\n",
    "    left=Side(border_style='thin', color=\"FF000000\"),\n",
    "    right=Side(border_style='thin', color=\"FF000000\")\n",
    ")\n",
    "\n",
    "# Set row heights and apply left alignment to all rows\n",
    "last_row = ws.max_row\n",
    "for i in range(2, last_row + 1):\n",
    "    ws.row_dimensions[i].height = 15\n",
    "\n",
    "# Apply number format to specific columns\n",
    "for col in [\"B\", \"AJ\"]:\n",
    "    for cell in ws[col]:\n",
    "        cell.number_format = numbers.FORMAT_NUMBER\n",
    "\n",
    "# Apply alignment, border, and wrapping to all cells\n",
    "for row in ws.iter_rows(min_row=1, max_row=last_row):\n",
    "    for cell in row:\n",
    "        cell.alignment = left_alignment  # Left alignment for all cells\n",
    "        cell.border = thin_border        # Thin border for all cells\n",
    "        cell.alignment = wrap_alignment  # Enable text wrapping for all cells\n",
    "\n",
    "# Apply font and fill to header (first row)\n",
    "for cell in ws[\"1:1\"]:\n",
    "    cell.font = font\n",
    "    cell.fill = fill\n",
    "\n",
    "# Freeze the top row and first column\n",
    "ws.freeze_panes = ws[\"B2\"]\n",
    "\n",
    "# Set a standard column width for all columns\n",
    "for col in ws.columns:\n",
    "    ws.column_dimensions[col[0].column_letter].width = 30\n",
    "\n",
    "# Save the formatted workbook\n",
    "styled_file_path = os.path.join(f\"../data/processed/amazon_retail_data_{current_date}_styled.xlsx\")\n",
    "wb.save(styled_file_path)\n",
    "\n",
    "print(f\"Styled Excel file saved to: {styled_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
